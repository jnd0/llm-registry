module.exports=[93695,(a,b,c)=>{b.exports=a.x("next/dist/shared/lib/no-fallback-error.external.js",()=>require("next/dist/shared/lib/no-fallback-error.external.js"))},95044,a=>{a.n(a.i(52425))},26087,a=>{a.n(a.i(21646))},10245,a=>{a.n(a.i(56873))},26659,a=>{a.n(a.i(10496))},50645,a=>{a.n(a.i(27572))},43619,a=>{a.n(a.i(79962))},13718,a=>{a.n(a.i(85523))},18198,a=>{a.n(a.i(45518))},62212,a=>{a.n(a.i(66114))},72549,a=>{"use strict";var b=a.i(20286);let c=[{id:"Intelligence",label:"Intelligence",description:"Reasoning, scientific understanding, and complex problem-solving abilities",longDescription:"Measures advanced cognitive capabilities including logical reasoning, scientific knowledge, multi-step problem solving, and the ability to tackle novel challenges. Includes benchmarks for GPQA, ARC-AGI, and other frontier reasoning tasks.",categories:["Reasoning","Science","STEM","Advanced Tasks"],icon:"Brain"},{id:"Knowledge & Communication",label:"Knowledge & Communication",description:"World knowledge, multilingual capabilities, and real-world understanding",longDescription:"Evaluates breadth and depth of world knowledge, language understanding across multiple languages, and ability to communicate effectively. Covers MMLU, HellaSwag, WMT translations, and real-world task performance.",categories:["Knowledge","Multilingual","Real-world"],icon:"Globe"},{id:"Coding",label:"Coding",description:"Code generation, software engineering, and programming tasks",longDescription:"Tests programming proficiency across multiple languages, software engineering tasks, debugging capabilities, and real-world coding scenarios. Includes HumanEval, MBPP, SWE-bench, and competitive programming benchmarks.",categories:["Coding"],icon:"Code"},{id:"Math",label:"Math",description:"Mathematical reasoning, competition math, and quantitative problem-solving",longDescription:"Assesses mathematical capabilities from basic arithmetic to competition-level problems. Covers GSM8K, MATH, AIME, and specialized mathematical reasoning benchmarks.",categories:["Math"],icon:"Calculator"},{id:"Agents & Tools",label:"Agents & Tools",description:"Tool use, agentic workflows, and instruction following",longDescription:"Measures ability to use external tools, follow complex instructions, operate autonomously in multi-step workflows, and function as effective AI agents. Includes BFCL, API-based tasks, and instruction following benchmarks.",categories:["Agent","Agentic","Instruction Following"],icon:"Bot"},{id:"Vision & Video",label:"Vision & Video",description:"Image understanding, video analysis, and multimodal capabilities",longDescription:"Evaluates visual understanding including image classification, object detection, video comprehension, and multimodal reasoning. Covers MMMU, VQA, video understanding, and cross-modal tasks.",categories:["Vision","Video","Multimodal"],icon:"Eye"},{id:"Long Context",label:"Long Context",description:"Performance on extended documents and long-context reasoning",longDescription:"Tests ability to process, understand, and reason over very long inputs. Includes needle-in-haystack tests, long-document QA, and benchmarks measuring performance degradation with context length.",categories:["Long Context"],icon:"Scroll"},{id:"Factuality",label:"Factuality",description:"Accuracy, hallucination resistance, and factual reliability",longDescription:"Measures tendency to produce factually accurate outputs, resistance to hallucinations, and ability to acknowledge uncertainty. Includes TruthfulQA, FACTSCORE, and other factuality benchmarks.",categories:["Hallucination"],icon:"CheckCircle"}];function d(a){var b=a.category;for(let a of c)if(a.categories.includes(b))return a.id;return null}function e(a){let d=c.find(b=>b.id===a);return d?b.benchmarks.filter(a=>d.categories.includes(a.category)):[]}function f(a){return e(a).map(a=>a.id)}c.reduce((a,b)=>(a[b.id]=b.categories,a),{});let g=c.map(a=>a.id);function h(a){return a.toLowerCase().replace(/\s*&\s*/g,"-").replace(/\s+/g,"-")}function i(a){for(let b of g)if(h(b)===a)return b;return null}a.s(["domainDefinitions",0,c,"domainToSlug",()=>h,"getBenchmarkIdsForDomain",()=>f,"getBenchmarksForDomain",()=>e,"getDomainForBenchmark",()=>d,"slugToDomain",()=>i])},69713,a=>{"use strict";let b=(0,a.i(1269).default)("external-link",[["path",{d:"M15 3h6v6",key:"1q9fwt"}],["path",{d:"M10 14 21 3",key:"gplh6r"}],["path",{d:"M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6",key:"a6xqqp"}]]);a.s(["ExternalLink",()=>b],69713)},70231,a=>{"use strict";var b=a.i(7997),c=a.i(54404),d=a.i(72549),e=a.i(95936),f=a.i(69713),g=a.i(96835);let h={title:"Benchmarks",description:"Explore all benchmarks tracked in the LLM Registry.",keywords:["llm benchmarks","ai evaluation benchmarks","benchmark categories","model testing datasets"],alternates:{canonical:"/benchmarks"},openGraph:{title:`LLM Benchmarks Directory | ${g.siteName}`,description:"Explore all benchmarks tracked in the LLM Registry.",url:`${g.siteUrl}/benchmarks`,type:"website",images:[{url:`${g.siteUrl}/opengraph-image.png`,width:1200,height:630,alt:"LLM benchmarks directory"}]},twitter:{card:"summary_large_image",title:`LLM Benchmarks Directory | ${g.siteName}`,description:"Explore all benchmarks tracked in the LLM Registry.",images:[`${g.siteUrl}/opengraph-image.png`]}};function i(){let a=new Map;for(let b of c.benchmarks){let c=(0,d.getDomainForBenchmark)(b)??"Other",e=a.get(c)??[];a.set(c,[...e,b])}let g=[...d.domainDefinitions.map(a=>a.id),"Other"];return(0,b.jsxs)("div",{className:"space-y-8",children:[(0,b.jsxs)("div",{children:[(0,b.jsx)("h1",{className:"font-display text-3xl font-bold tracking-tight text-foreground",children:"Benchmarks"}),(0,b.jsxs)("p",{className:"mt-2 text-muted-foreground",children:["Explore ",c.benchmarks.length," benchmarks across ",d.domainDefinitions.length," capability domains."]})]}),g.map(c=>{let g=a.get(c);if(!g||0===g.length)return null;let h=d.domainDefinitions.find(a=>a.id===c);return(0,b.jsxs)("section",{className:"space-y-4",children:[(0,b.jsxs)("div",{className:"flex items-center gap-3",children:[(0,b.jsx)("h2",{className:"font-display text-xl font-bold tracking-tight text-foreground",children:h?.label??c}),(0,b.jsx)("span",{className:"rounded-full bg-muted px-2.5 py-0.5 text-[10px] font-bold text-muted-foreground",children:g.length})]}),h&&(0,b.jsx)("p",{className:"text-sm text-muted-foreground",children:h.description}),(0,b.jsx)("div",{className:"grid gap-3 sm:grid-cols-2 lg:grid-cols-3",children:g.map(a=>(0,b.jsxs)(e.default,{href:`/benchmark/${a.id}`,className:"group surface-card rounded-xl border border-border/40 p-4 transition-all hover:border-primary/30 hover:shadow-lg hover:shadow-primary/5",children:[(0,b.jsxs)("div",{className:"flex items-start justify-between gap-2",children:[(0,b.jsx)("h3",{className:"font-display text-sm font-bold text-foreground group-hover:text-primary transition-colors",children:a.name}),(0,b.jsx)("span",{className:"rounded-md bg-muted px-1.5 py-0.5 text-[9px] font-bold uppercase tracking-wider text-muted-foreground",children:a.category})]}),(0,b.jsx)("p",{className:"mt-2 text-xs text-muted-foreground line-clamp-2",children:a.description}),(0,b.jsxs)("div",{className:"mt-3 flex items-center justify-between",children:[(0,b.jsxs)("span",{className:"font-mono text-[10px] text-muted-foreground",children:["Unit: ",a.unit??"%"]}),(0,b.jsxs)("div",{className:"flex items-center gap-2 opacity-0 group-hover:opacity-100 transition-opacity",children:[a.link&&(0,b.jsxs)("span",{className:"inline-flex items-center gap-1 text-[10px] text-primary",children:["Website",(0,b.jsx)(f.ExternalLink,{className:"h-2.5 w-2.5"})]}),a.paperUrl&&(0,b.jsxs)("span",{className:"inline-flex items-center gap-1 text-[10px] text-primary",children:["Paper",(0,b.jsx)(f.ExternalLink,{className:"h-2.5 w-2.5"})]})]})]})]},a.id))})]},c)})]})}a.s(["default",()=>i,"metadata",0,h])}];

//# sourceMappingURL=%5Broot-of-the-server%5D__24eb6633._.js.map