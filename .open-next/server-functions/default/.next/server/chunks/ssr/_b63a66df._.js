module.exports=[29947,a=>{"use strict";let b={mmlu:"https://arxiv.org/abs/2009.03300","mmlu-pro":"https://arxiv.org/abs/2406.01574",math:"https://arxiv.org/abs/2103.03874","human-eval":"https://arxiv.org/abs/2107.03374","swe-bench-verified":"https://www.swebench.com/",mmmu:"https://mmmu-benchmark.github.io/","mmmu-pro":"https://mmmu-benchmark.github.io/","mmmu-val":"https://mmmu-benchmark.github.io/","mmmu-vision":"https://mmmu-benchmark.github.io/",mmmlu:"https://arxiv.org/abs/2402.03300",mathvista:"https://mathvista.github.io/","mathvista-mini":"https://mathvista.github.io/",livebench:"https://livebench.ai/#/","livecodebench-v6":"https://livecodebench.github.io/","livecodebench-pro":"https://livecodebenchpro.com/",bigcodebench:"https://github.com/bigcode-project/bigcodebench",gsm8k:"https://github.com/openai/grade-school-math",aime:"https://artofproblemsolving.com/wiki/index.php/AIME","aime-2025":"https://artofproblemsolving.com/wiki/index.php/AIME","aime-2026":"https://artofproblemsolving.com/wiki/index.php/AIME","gpqa-diamond":"https://arxiv.org/abs/2311.12022","arc-agi-1":"https://arcprize.org/","arc-agi-2":"https://arcprize.org/","arc-agi-1-image":"https://arcprize.org/","arc-agi-2-image":"https://arcprize.org/",hle:"https://agi.safe.ai/","hle-full":"https://agi.safe.ai/","hle-full-tools":"https://agi.safe.ai/","hle-text":"https://agi.safe.ai/","hle-verified":"https://agi.safe.ai/","hle-vl":"https://agi.safe.ai/",codeforces:"https://codeforces.com/","lmarena-elo":"https://chat.lmsys.org/?leaderboard",webarena:"https://webarena.dev/",agentbench:"https://github.com/THUDM/AgentBench","tau-bench":"https://github.com/sierra-research/tau2-bench","tau-bench-retail":"https://taubench.com/","tau-bench-telecom":"https://github.com/sierra-research/tau2-bench",docvqa:"https://rrc.cvc.uab.es/?ch=17",ocrbench:"https://ocrbench.github.io/","ocrbench-v2":"https://ocrbench.github.io/",videomme:"https://video-mme.github.io/","terminal-bench":"https://www.tbench.ai/leaderboard/terminal-bench/2.0","terminal-bench-hard":"https://www.tbench.ai/leaderboard/terminal-bench/2.0","osworld-verified":"https://os-world.github.io/","swe-bench-pro":"https://www.swebench.com/",lcr:"https://artificialanalysis.ai/evaluations#lcr",ifbench:"https://artificialanalysis.ai/evaluations#ifbench","aa-omniscience":"https://artificialanalysis.ai/evaluations#omniscience",critpt:"https://arxiv.org/abs/2501.00663",scicode:"https://github.com/scicode-bench/scicode",browsecomp:"https://github.com/google-research/browsecomp","screenspot-pro":"https://github.com/octopus-tools/screenspot-pro","charxiv-reasoning":"https://arxiv.org/abs/2406.18521","facts-benchmark":"https://github.com/google-deepmind/facts-benchmark","mcp-atlas":"https://github.com/modelcontextprotocol/mcp-atlas",toolathlon:"https://github.com/toolathlon/toolathlon"},c=[{id:"mmlu",name:"MMLU (5-shot)",category:"Knowledge",description:"Massive Multitask Language Understanding covers 57 subjects across STEM, the humanities, social sciences, and more.",maxScore:100,higherIsBetter:!0,link:"https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu",paperUrl:"https://arxiv.org/abs/2009.03300"},{id:"math",name:"MATH (CoT)",category:"Math",description:"Challenging competition mathematics problems (AIME/IMO level).",maxScore:100,higherIsBetter:!0,link:"https://paperswithcode.com/sota/math-word-problem-solving-on-math",paperUrl:"https://arxiv.org/abs/2103.03874"},{id:"human-eval",name:"HumanEval",category:"Coding",description:"Functional correctness of synthesized programs from docstrings.",maxScore:100,higherIsBetter:!0,link:"https://paperswithcode.com/sota/code-generation-on-humaneval",paperUrl:"https://arxiv.org/abs/2107.03374"},{id:"swe-bench-verified",name:"SWE-bench Verified",category:"Coding",description:"Resolving real-world GitHub issues. Verified subset ensures solvable issues.",maxScore:100,higherIsBetter:!0,link:"https://www.swebench.com/",paperUrl:"https://arxiv.org/abs/2310.06770"},{id:"mmmu",name:"MMMU (Multimodal)",category:"Multimodal",description:"Multi-discipline Multimodal Understanding and Reasoning.",maxScore:100,higherIsBetter:!0,link:"https://mmmu-benchmark.github.io/",paperUrl:"https://arxiv.org/abs/2311.16502"},{id:"livebench",name:"LiveBench",category:"Reasoning",description:"Contamination-free, continuously updated reasoning benchmark.",maxScore:100,higherIsBetter:!0,link:"https://livebench.ai/#/",paperUrl:"https://arxiv.org/abs/2406.19314"},{id:"bigcodebench",name:"BigCodeBench",category:"Coding",description:"Next-generation HumanEval with more diverse library calls and complex tasks.",maxScore:100,higherIsBetter:!0,link:"https://github.com/bigcode-project/bigcodebench",paperUrl:"https://arxiv.org/abs/2406.15877"},{id:"gsm8k",name:"GSM8K",category:"Math",description:"Grade school math word problems requiring multi-step reasoning.",maxScore:100,higherIsBetter:!0,link:"https://github.com/openai/grade-school-math",paperUrl:"https://arxiv.org/abs/2110.14168"},{id:"aime",name:"AIME 2024/25",category:"Math",description:"American Invitational Mathematics Examination. Competition-level math.",maxScore:100,higherIsBetter:!0},{id:"lmarena-elo",name:"LMArena ELO",category:"Real-world",description:"Chatbot Arena ELO score. Crowd-sourced human preference ranking.",maxScore:1700,minScore:1e3,higherIsBetter:!0,normalization:"elo",unit:"ELO",link:"https://chat.lmsys.org/?leaderboard"},{id:"aa-intelligence-index",name:"AA Intelligence Index",category:"Real-world",description:"Artificial Analysis aggregate intelligence index.",maxScore:100,higherIsBetter:!0},{id:"agentbench",name:"AgentBench",category:"Agent",description:"Comprehensive framework to evaluate LLMs as agents across diverse environments.",maxScore:100,higherIsBetter:!0,link:"https://github.com/THUDM/AgentBench",paperUrl:"https://arxiv.org/abs/2308.03688"},{id:"mmlu-pro",name:"MMLU-Pro",category:"Science",description:"A more robust and harder version of MMLU, focusing on complex reasoning and STEM subjects.",maxScore:100,higherIsBetter:!0,link:"https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro",paperUrl:"https://arxiv.org/abs/2406.01574"},{id:"hle",name:"HLE",category:"Science",description:"Humanity's Last Exam - Hard reasoning benchmark without tools.",maxScore:100,higherIsBetter:!0,link:"https://agi.safe.ai/",paperUrl:"https://arxiv.org/abs/2501.14249"},{id:"hle-full",name:"HLE-Full",category:"Science",description:"Humanity's Last Exam full evaluation without tools.",maxScore:100,higherIsBetter:!0,link:"https://agi.safe.ai/",paperUrl:"https://arxiv.org/abs/2501.14249"},{id:"hle-full-tools",name:"HLE-Full (w/ tools)",category:"Science",description:"Humanity's Last Exam full evaluation with tool access enabled.",maxScore:100,higherIsBetter:!0,link:"https://agi.safe.ai/",paperUrl:"https://arxiv.org/abs/2501.14249"},{id:"critpt",name:"CritPt",category:"Science",description:"Complex Research using Integrated Thinking - Physics Test. Research-level physics reasoning.",maxScore:100,higherIsBetter:!0,link:"https://arxiv.org/abs/2501.00663"},{id:"simpleqa",name:"SimpleQA",category:"Science",description:"Open-domain factuality benchmark focusing on short, verifiable answers.",maxScore:100,higherIsBetter:!0},{id:"simpleqa-verified",name:"SimpleQA Verified",category:"Knowledge",description:"Verified subset of SimpleQA for parametric knowledge evaluation.",maxScore:100,higherIsBetter:!0},{id:"healthbench",name:"HealthBench",category:"Science",description:"Medical knowledge and diagnostic reasoning evaluation.",maxScore:100,higherIsBetter:!0},{id:"supergpqa",name:"SuperGPQA",category:"Science",description:"Extremely difficult expert-level science questions.",maxScore:100,higherIsBetter:!0},{id:"aime-2026",name:"AIME 2026",category:"Math",description:"Future prediction of AIME performance levels.",maxScore:100,higherIsBetter:!0},{id:"aa-math-index",name:"AA Math Index",category:"Math",description:"Artificial Analysis aggregate math capability index.",maxScore:100,higherIsBetter:!0},{id:"hmmt-feb-2025",name:"HMMT Feb 2025",category:"Math",description:"Harvard-MIT Mathematics Tournament - High difficulty competition math.",maxScore:100,higherIsBetter:!0},{id:"math-500",name:"MATH-500",category:"Math",description:"500-problem math benchmark for broad quantitative reasoning.",maxScore:100,higherIsBetter:!0},{id:"imo-answerbench",name:"IMO-AnswerBench",category:"Math",description:"International Mathematical Olympiad style answer-only benchmark.",maxScore:100,higherIsBetter:!0},{id:"matharena-apex",name:"MathArenaApex",category:"Math",description:"Competitive math arena for top-tier reasoning models.",maxScore:100,higherIsBetter:!0},{id:"codeforces",name:"Codeforces",category:"Coding",description:"Competitive programming rating based on problem solving.",maxScore:4e3,higherIsBetter:!0,normalization:"max",unit:"rating"},{id:"livecodebench-v6",name:"LiveCodeBench v6",category:"Coding",description:"Contamination-free coding benchmark using recent problems.",maxScore:100,higherIsBetter:!0,link:"https://livecodebench.github.io/",paperUrl:"https://arxiv.org/abs/2403.07974"},{id:"livecodebench-pro",name:"LiveCodeBench Pro",category:"Coding",description:"Competitive programming problems from Codeforces, ICPC, and IOI with Elo rating.",maxScore:4e3,higherIsBetter:!0,normalization:"max",unit:"Elo"},{id:"aa-coding-index",name:"AA Coding Index",category:"Coding",description:"Artificial Analysis aggregate coding capability index.",maxScore:100,higherIsBetter:!0},{id:"paperbench-codedev",name:"PaperBench (CodeDev)",category:"Coding",description:"Research-grade coding and software development tasks.",maxScore:100,higherIsBetter:!0},{id:"cybergym",name:"CyberGym",category:"Coding",description:"Cybersecurity-flavored coding benchmark in simulated environments.",maxScore:100,higherIsBetter:!0},{id:"ojbench-cpp",name:"OJBench (cpp)",category:"Coding",description:"Online-judge competitive coding benchmark focused on C++ tasks.",maxScore:100,higherIsBetter:!0},{id:"gpqa-diamond",name:"GPQA Diamond",category:"STEM",description:"Graduate-Level Google-Proof Q&A Benchmark.",maxScore:100,higherIsBetter:!0,link:"https://gpqa-benchmark.github.io/",paperUrl:"https://arxiv.org/abs/2311.12022"},{id:"phybench",name:"Phybench",category:"STEM",description:"Physics reasoning and problem solving benchmark.",maxScore:100,higherIsBetter:!0},{id:"arc-agi-1",name:"ARC-AGI-1",category:"Reasoning",description:"Abstraction and Reasoning Corpus - Level 1.",maxScore:100,higherIsBetter:!0,link:"https://arcprize.org/",paperUrl:"https://arxiv.org/abs/1911.01547"},{id:"mrcr-v2",name:"MRCR v2",category:"Long Context",description:"Multi-Round Context Retrieval - 8-needle test.",maxScore:100,higherIsBetter:!0},{id:"longbench-v2",name:"LongBench v2",category:"Long Context",description:"Comprehensive long-context understanding (128k).",maxScore:100,higherIsBetter:!0},{id:"lcr",name:"AA-LCR",category:"Long Context",description:"Artificial Analysis Long Context Reasoning benchmark. Evaluates reasoning over long contexts.",maxScore:100,higherIsBetter:!0,link:"https://artificialanalysis.ai/evaluations#lcr"},{id:"mmmlu",name:"MMMLU",category:"Multilingual",description:"Massive Multilingual Language Understanding.",maxScore:100,higherIsBetter:!0,paperUrl:"https://arxiv.org/abs/2402.03300"},{id:"ifeval",name:"IFEval",category:"Instruction Following",description:"Instruction Following Evaluation for Large Language Models. Measures ability to follow strict formatting and constraint requirements.",maxScore:100,higherIsBetter:!0,link:"https://huggingface.co/spaces/yejinxie/IFEval",paperUrl:"https://arxiv.org/abs/2311.07911"},{id:"ifeval-inverse",name:"Inverse IFEval",category:"Instruction Following",description:"Reverse instruction following evaluation.",maxScore:100,higherIsBetter:!0},{id:"ifbench",name:"IFBench",category:"Instruction Following",description:"Artificial Analysis IFBench. Evaluates precise instruction following with constraints.",maxScore:100,higherIsBetter:!0,link:"https://artificialanalysis.ai/evaluations#ifbench"},{id:"verified-advancedif",name:"Verified AdvancedIF",category:"Instruction Following",description:"Advanced instruction-following benchmark with verified grading.",maxScore:100,higherIsBetter:!0},{id:"longfact-concepts",name:"LongFact-Concepts",category:"Hallucination",description:"Factuality in long-form conceptual generations.",maxScore:100,higherIsBetter:!0},{id:"aa-omniscience",name:"AA-Omniscience",category:"Hallucination",description:"Evaluates model omniscience and factual reliability across diverse domains.",maxScore:100,minScore:-100,higherIsBetter:!0,link:"https://artificialanalysis.ai/evaluations#omniscience"},{id:"aime-2025",name:"AIME 2025",category:"Math",description:"American Invitational Mathematics Examination 2025 problems.",maxScore:100,higherIsBetter:!0},{id:"arc-agi-2",name:"ARC-AGI-2",category:"Reasoning",description:"Abstraction and Reasoning Corpus - Level 2 (Extreme difficulty).",maxScore:100,higherIsBetter:!0},{id:"factscore",name:"FactScore",category:"Hallucination",description:"Precision of fine-grained facts in long-form biographies.",maxScore:100,higherIsBetter:!0},{id:"superchem",name:"Superchem",category:"STEM",description:"Expert-level chemistry knowledge and reasoning.",maxScore:100,higherIsBetter:!0},{id:"korbench",name:"KORBench",category:"Reasoning",description:"Korean reasoning and language understanding benchmark.",maxScore:100,higherIsBetter:!0},{id:"graphwalks-bfs",name:"Graphwalks Bfs",category:"Long Context",description:"Traversal-based long context reasoning using BFS (128k).",maxScore:100,higherIsBetter:!0},{id:"global-piqa",name:"Global PIQA",category:"Multilingual",description:"Physical Interaction QA across multiple languages and cultures.",maxScore:100,higherIsBetter:!0},{id:"multichallenge",name:"MultiChallenge",category:"Instruction Following",description:"Complex, multi-constraint instruction following tasks.",maxScore:100,higherIsBetter:!0},{id:"longfact-objects",name:"LongFact-Objects",category:"Hallucination",description:"Factuality in long-form generations about objects.",maxScore:100,higherIsBetter:!0},{id:"putnam-200",name:"Putnam-200",category:"Math",description:"William Lowell Putnam Mathematical Competition problems - top 200 level difficulty.",maxScore:100,higherIsBetter:!0},{id:"mathvista",name:"MathVista",category:"Vision",description:"Mathematical reasoning in visual contexts.",maxScore:100,higherIsBetter:!0,link:"https://mathvista.github.io/",paperUrl:"https://arxiv.org/abs/2310.02255"},{id:"mathvista-mini",name:"MathVista (mini)",category:"Vision",description:"Compact MathVista split for faster multimodal reasoning checks.",maxScore:100,higherIsBetter:!0},{id:"mathvision",name:"MathVision",category:"Vision",description:"Comprehensive mathematical vision benchmark.",maxScore:100,higherIsBetter:!0},{id:"mmmu-vision",name:"MMMU",category:"Vision",description:"Massive Multi-discipline Multimodal Understanding and Reasoning.",maxScore:100,higherIsBetter:!0},{id:"logicvista",name:"LogicVista",category:"Vision",description:"Logical reasoning in visual puzzles and diagrams.",maxScore:100,higherIsBetter:!0},{id:"blink",name:"BLINK",category:"Vision",description:"Spatial and perception benchmark for multimodal models.",maxScore:100,higherIsBetter:!0},{id:"mmvp",name:"MMVP",category:"Vision",description:"Multimodal visual perception benchmark.",maxScore:100,higherIsBetter:!0},{id:"chartqapro",name:"ChartQA Pro",category:"Vision",description:"Expert-level chart understanding and question answering.",maxScore:100,higherIsBetter:!0},{id:"docvqa",name:"DocVQA",category:"Vision",description:"Document visual question answering on scanned and digital documents.",maxScore:100,higherIsBetter:!0,link:"https://rrc.cvc.uab.es/?ch=17",paperUrl:"https://arxiv.org/abs/2007.00398"},{id:"ocrbench-v2",name:"OCRBench v2",category:"Vision",description:"Next-gen optical character recognition and document understanding.",maxScore:100,higherIsBetter:!0},{id:"ocrbench",name:"OCRBench",category:"Vision",description:"Optical character recognition and document understanding benchmark.",maxScore:100,higherIsBetter:!0,link:"https://ocrbench.github.io/",paperUrl:"https://arxiv.org/abs/2312.16151"},{id:"dynamath",name:"DynaMath",category:"Vision",description:"Dynamic mathematical reasoning in visual contexts.",maxScore:100,higherIsBetter:!0},{id:"mathkangaroo",name:"MathKangaroo",category:"Vision",description:"Mathematical competition problems with visual elements.",maxScore:100,higherIsBetter:!0},{id:"mathcanvas",name:"MathCanvas",category:"Vision",description:"Multi-step mathematical reasoning on a canvas.",maxScore:100,higherIsBetter:!0},{id:"mmmu-pro",name:"MMMU-Pro",category:"Vision",description:"Professional level MMMU expansion.",maxScore:100,higherIsBetter:!0},{id:"mmmu-val",name:"MMMU (val)",category:"Vision",description:"Validation split of MMMU for multimodal understanding.",maxScore:100,higherIsBetter:!0},{id:"emma",name:"EMMA",category:"Vision",description:"Expert-level Multimodal Mathematics Analysis.",maxScore:100,higherIsBetter:!0},{id:"sfe",name:"SFE",category:"Vision",description:"Scientific Figure Evaluation.",maxScore:100,higherIsBetter:!0},{id:"hipho",name:"HiPhO",category:"Vision",description:"High-level Physics Olympiad (Vision).",maxScore:100,higherIsBetter:!0},{id:"xlrs-bench",name:"XLRS-Bench",category:"Vision",description:"Cross-domain Logical Reasoning and Spatial benchmark.",maxScore:100,higherIsBetter:!0},{id:"phyx",name:"PhyX",category:"Vision",description:"Physics reasoning with open-ended visual questions.",maxScore:100,higherIsBetter:!0},{id:"vpct",name:"VPCT",category:"Vision",description:"Visual Perception and Coding Tasks.",maxScore:100,higherIsBetter:!0},{id:"zerobench-main",name:"ZeroBench (main)",category:"Vision",description:"Zero-shot visual reasoning benchmark.",maxScore:100,higherIsBetter:!0},{id:"zerobench-sub",name:"ZeroBench (sub)",category:"Vision",description:"Zero-shot visual reasoning sub-tasks.",maxScore:100,higherIsBetter:!0},{id:"zerobench",name:"ZeroBench",category:"Vision",description:"Aggregate ZeroBench score across the full task set.",maxScore:100,higherIsBetter:!0},{id:"zerobench-tools",name:"ZeroBench (w/ tools)",category:"Vision",description:"ZeroBench score when tool use is allowed.",maxScore:100,higherIsBetter:!0},{id:"arc-agi-1-image",name:"ArcAGI1-Image",category:"Vision",description:"ARC-AGI Level 1 tasks in image format.",maxScore:100,higherIsBetter:!0},{id:"arc-agi-2-image",name:"ArcAGI2-Image",category:"Vision",description:"ARC-AGI Level 2 tasks in image format.",maxScore:100,higherIsBetter:!0},{id:"visulogic",name:"VisuLogic",category:"Vision",description:"Visual logic and sequence reasoning.",maxScore:100,higherIsBetter:!0},{id:"vlms-are-biased",name:"VLMsAreBiased",category:"Vision",description:"Evaluating bias in Vision-Language Models.",maxScore:100,higherIsBetter:!0},{id:"vlms-are-blind",name:"VLMsAreBlind",category:"Vision",description:"Evaluating perception failures in VLMs.",maxScore:100,higherIsBetter:!0},{id:"visfactor",name:"VisFactor",category:"Vision",description:"Visual factor identification and reasoning.",maxScore:100,higherIsBetter:!0},{id:"realworldqa",name:"RealWorldQA",category:"Vision",description:"Real-world visual question answering.",maxScore:100,higherIsBetter:!0},{id:"babyvision",name:"BabyVision",category:"Vision",description:"Early-stage visual development benchmark.",maxScore:100,higherIsBetter:!0},{id:"hallusionbench",name:"HallusionBench",category:"Vision",description:"Visual hallucination and factuality benchmark.",maxScore:100,higherIsBetter:!0},{id:"mme-cc",name:"MME-CC",category:"Vision",description:"Multimodal Evaluation (Cognitive Capacity).",maxScore:100,higherIsBetter:!0},{id:"mmstar",name:"MMStar",category:"Vision",description:"Elite multimodal model evaluation.",maxScore:100,higherIsBetter:!0},{id:"muirbench",name:"MUIRBench",category:"Vision",description:"Multimodal Understanding and Interaction Benchmark.",maxScore:100,higherIsBetter:!0},{id:"mtvqa",name:"MTVQA",category:"Vision",description:"Multilingual Text-centric Visual QA.",maxScore:100,higherIsBetter:!0},{id:"worldvqa",name:"WorldVQA",category:"Vision",description:"Global visual knowledge and reasoning.",maxScore:100,higherIsBetter:!0},{id:"vibeeval",name:"VibeEval",category:"Vision",description:"Subjective and intuitive visual quality evaluation.",maxScore:100,higherIsBetter:!0},{id:"viverbench",name:"ViVerBench",category:"Vision",description:"Visual Verification and reasoning.",maxScore:100,higherIsBetter:!0},{id:"countbench",name:"CountBench",category:"Vision",description:"Visual object counting and identification.",maxScore:100,higherIsBetter:!0},{id:"fsc-147",name:"FSC-147",category:"Vision",description:"Few-shot counting benchmark (Lower is better handled in normalization).",maxScore:100,higherIsBetter:!1,normalization:"inverse",unit:"error"},{id:"point-bench",name:"Point-Bench",category:"Vision",description:"Visual pointing and spatial grounding.",maxScore:100,higherIsBetter:!0},{id:"mmsibench",name:"MMSIBench",category:"Vision",description:"Multimodal Spatial Interaction Benchmark.",maxScore:100,higherIsBetter:!0},{id:"treebench",name:"TreeBench",category:"Vision",description:"Hierarchical visual reasoning tasks.",maxScore:100,higherIsBetter:!0},{id:"refspatialbench",name:"RefSpatialBench",category:"Vision",description:"Referential spatial reasoning evaluation.",maxScore:100,higherIsBetter:!0},{id:"da-2k",name:"DA-2K",category:"Vision",description:"Document Analysis and reasoning (2k).",maxScore:100,higherIsBetter:!0},{id:"all-angles",name:"All-Angles",category:"Vision",description:"Multi-perspective visual understanding.",maxScore:100,higherIsBetter:!0},{id:"erqa",name:"ERQA",category:"Vision",description:"Environment Reasoning and Question Answering.",maxScore:100,higherIsBetter:!0},{id:"omnidocbench",name:"OmniDocBench",category:"Vision",description:"Universal document understanding benchmark.",maxScore:100,higherIsBetter:!0},{id:"omnidocbench-15",name:"OmniDocBench 1.5",category:"Vision",description:"OCR benchmark measuring edit distance (lower is better).",maxScore:1,minScore:0,higherIsBetter:!1,unit:"edit distance"},{id:"screenspot-pro",name:"ScreenSpot-Pro",category:"Vision",description:"Screen understanding benchmark for GUI interaction.",maxScore:100,higherIsBetter:!0},{id:"infovqa-test",name:"InfoVQA (test)",category:"Vision",description:"Information-seeking visual question answering on the test split.",maxScore:100,higherIsBetter:!0},{id:"charxiv-dq",name:"CharXiv-DQ",category:"Vision",description:"Chart-based reasoning from arXiv papers (Data QA).",maxScore:100,higherIsBetter:!0},{id:"charxiv-rq",name:"CharXiv-RQ",category:"Vision",description:"Chart-based reasoning from arXiv papers (Reasoning QA).",maxScore:100,higherIsBetter:!0},{id:"charxiv-reasoning",name:"CharXiv Reasoning",category:"Vision",description:"Information synthesis from complex charts.",maxScore:100,higherIsBetter:!0},{id:"dude",name:"DUDE",category:"Vision",description:"Document Understanding and Dialogue Evaluation.",maxScore:100,higherIsBetter:!0},{id:"mmlongbench",name:"MMLongBench",category:"Vision",description:"Multimodal Long context benchmark.",maxScore:100,higherIsBetter:!0},{id:"longdocurl",name:"LongDocURL",category:"Vision",description:"Long document understanding with URLs.",maxScore:100,higherIsBetter:!0},{id:"mmlongbench-doc",name:"MMLongBench-Doc",category:"Vision",description:"Multimodal Long context document evaluation.",maxScore:100,higherIsBetter:!0},{id:"mmvu",name:"MMVU",category:"Video",description:"Multimodal Video Understanding.",maxScore:100,higherIsBetter:!0},{id:"videosimpleqa",name:"VideoSimpleQA",category:"Video",description:"Verifiable question answering for short video clips.",maxScore:100,higherIsBetter:!0},{id:"videoreasonbench",name:"VideoReasonBench",category:"Video",description:"Complex reasoning tasks in video content.",maxScore:100,higherIsBetter:!0},{id:"morse-500",name:"Morse-500",category:"Video",description:"Sequence reasoning and motion understanding.",maxScore:100,higherIsBetter:!0},{id:"videoholmes",name:"VideoHolmes",category:"Video",description:"Deep diagnostic video understanding.",maxScore:100,higherIsBetter:!0},{id:"minerva",name:"Minerva",category:"Video",description:"Long-form video reasoning and knowledge retrieval.",maxScore:100,higherIsBetter:!0},{id:"contphy",name:"ContPhy",category:"Video",description:"Continuous Physics reasoning in video.",maxScore:100,higherIsBetter:!0},{id:"tempcompass",name:"TempCompass",category:"Video",description:"Temporal orientation and perception in video.",maxScore:100,higherIsBetter:!0},{id:"egotempo",name:"EgoTempo",category:"Video",description:"First-person perspective temporal reasoning.",maxScore:100,higherIsBetter:!0},{id:"motionbench",name:"MotionBench",category:"Video",description:"Comprehensive motion perception evaluation.",maxScore:100,higherIsBetter:!0},{id:"tomato",name:"TOMATO",category:"Video",description:"Temporal Object-centric Multimodal Analysis.",maxScore:100,higherIsBetter:!0},{id:"cgbench",name:"CGBench",category:"Video",description:"Contextual Grounding in long videos.",maxScore:100,higherIsBetter:!0},{id:"longvideobench",name:"LongVideoBench",category:"Video",description:"Understanding extremely long-form video content.",maxScore:100,higherIsBetter:!0},{id:"videoeval-pro",name:"VideoEval-Pro",category:"Video",description:"Professional level video quality and content evaluation.",maxScore:100,higherIsBetter:!0},{id:"lvbench",name:"LVBench",category:"Video",description:"Large-scale Video Benchmark.",maxScore:100,higherIsBetter:!0},{id:"crossvid",name:"CrossVid",category:"Video",description:"Cross-video temporal and relational reasoning.",maxScore:100,higherIsBetter:!0},{id:"livesports-3k",name:"LiveSports-3K",category:"Video",description:"Live sports broadcast understanding.",maxScore:100,higherIsBetter:!0},{id:"ovobench",name:"OVOBench",category:"Video",description:"Object-Video-Object relational reasoning.",maxScore:100,higherIsBetter:!0},{id:"odvbench",name:"ODVBench",category:"Video",description:"Open-Domain Video understanding.",maxScore:100,higherIsBetter:!0},{id:"vispeak",name:"ViSpeak",category:"Video",description:"Video-to-speech and dialogue reasoning.",maxScore:100,higherIsBetter:!0},{id:"frontiersci-olympiad",name:"FrontierSci-olympiad",category:"STEM",description:"Scientific Olympiad level problems.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"terminal-bench",name:"Terminal-Bench 2.0",category:"Agentic",description:"Agent performance in realistic terminal workflows (v2.0 leaderboard).",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"terminal-bench-hard",name:"Terminal-Bench Hard",category:"Agentic",description:"Hard split of Terminal-Bench focused on tougher terminal workflows.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"osworld-verified",name:"OSWorld-Verified",category:"Agentic",description:"Verified desktop computer-use benchmark for end-to-end task completion.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"webarena",name:"WebArena",category:"Agentic",description:"Browser-based autonomous task execution benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%",link:"https://webarena.dev/",paperUrl:"https://arxiv.org/abs/2307.13854"},{id:"swe-lancer",name:"SWE-Lancer",category:"Agentic",description:"Software engineering task completion in multi-step coding workflows.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"multi-swe-bench",name:"Multi-SWE-bench",category:"Agentic",description:"Multi-repository software engineering benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"swe-bench-pro",name:"SWE-bench Pro",category:"Agentic",description:"Higher-difficulty SWE-bench subset for frontier coding agents.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"swe-multilingual",name:"SWE Multilingual",category:"Agentic",description:"Software engineering performance across multilingual codebases.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"swe-evo",name:"SWE-Evo",category:"Agentic",description:"Evolutionary coding benchmark focused on long-horizon bug fixing.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"aider-polyglot",name:"Aider Polyglot",category:"Agentic",description:"Multi-language coding agent benchmark with editor-in-the-loop tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"artifactsbench",name:"ArtifactsBench",category:"Agentic",description:"Agent ability to produce complete, runnable software artifacts.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"codesimpleqa",name:"CodeSimpleQA",category:"Agentic",description:"Short-form coding QA with executable correctness checks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"spreadsheetbench-verified",name:"SpreadsheetBench Verified",category:"Agentic",description:"Verified spreadsheet manipulation and reasoning tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"browsecomp",name:"BrowseComp",category:"Agentic",description:"Web browsing + synthesis benchmark for research agents.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"browsecomp-ctx-manage",name:"BrowseComp (ctx manage)",category:"Agentic",description:"BrowseComp variant with explicit context-window management.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"browsecomp-agent-swarm",name:"BrowseComp (Agent Swarm)",category:"Agentic",description:"Multi-agent swarm variant of BrowseComp.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"browsecomp-zh",name:"BrowseComp-ZH",category:"Agentic",description:"Chinese-language browsing and synthesis benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"hle-text",name:"HLE-Text",category:"Agentic",description:"Text-only variant of Humanity's Last Exam under agentic settings.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"hle-verified",name:"HLE-Verified",category:"Agentic",description:"Verified subset of Humanity's Last Exam for reproducible evaluation.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"widesearch",name:"WideSearch",category:"Agentic",description:"Broad retrieval and synthesis benchmark across many sources.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"widesearch-agent-swarm",name:"WideSearch (Agent Swarm)",category:"Agentic",description:"Multi-agent swarm variant of WideSearch.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"finsearchcomp",name:"FinSearchComp",category:"Agentic",description:"Finance-focused search and evidence-grounded answering benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"finsearchcomp-t2-t3",name:"FinSearchComp T2&T3",category:"Agentic",description:"Tier 2 and Tier 3 slices of FinSearchComp.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"vending-bench-2",name:"Vending-Bench 2",category:"Agentic",description:"Long-horizon business simulation benchmark (final account balance).",maxScore:1e4,minScore:0,higherIsBetter:!0,normalization:"max",unit:"USD"},{id:"facts-benchmark",name:"FACTS Benchmark Suite",category:"Agentic",description:"Factuality benchmark across grounding, parametric, search, and multimodal.",maxScore:100,higherIsBetter:!0},{id:"mcp-atlas",name:"MCP Atlas",category:"Agentic",description:"Multi-step workflows using Model Context Protocol.",maxScore:100,higherIsBetter:!0},{id:"toolathlon",name:"Toolathlon",category:"Agentic",description:"Long horizon real-world software tasks.",maxScore:100,higherIsBetter:!0},{id:"deepsearchqa",name:"DeepSearchQA",category:"Agentic",description:"Deep multi-hop search QA for long-horizon agents.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"seal-0",name:"SEAL-0",category:"Agentic",description:"Strategic environment-agent loop benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"gdpval-aa",name:"GDPVal-AA",category:"Agentic",description:"Artificial Analysis GDPVal benchmark for knowledge-work quality.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tau-bench",name:"TAU-Bench",category:"Agentic",description:"Tool-use and API orchestration benchmark for assistants.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tau-bench-retail",name:"TAU-Bench Retail",category:"Agentic",description:"Retail-domain tool-use and workflow benchmark from τ²-bench.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tau-bench-telecom",name:"TAU-Bench Telecom",category:"Agentic",description:"Telecom-domain tool-use and workflow benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"mcp-mark",name:"MCP-Mark",category:"Agentic",description:"Model Context Protocol interoperability benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"bfcl-v4",name:"BFCL v4",category:"Agentic",description:"Function calling reliability benchmark (v4).",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"vitabench",name:"VitaBench",category:"Agentic",description:"Virtual task assistant benchmark across practical workflows.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"deepconsult",name:"DeepConsult",category:"Agentic",description:"Consulting-style multi-step reasoning and recommendation benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"deepresearchbench",name:"DeepResearchBench",category:"Agentic",description:"Long-horizon research task benchmark with citation requirements.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"researchrubrics",name:"ResearchRubrics",category:"Agentic",description:"Rubric-based evaluation of research quality and rigor.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"minedojo-verified",name:"MineDojo Verified",category:"Agentic",description:"Verified embodied-agent benchmark in Minecraft-style tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"mm-browsecomp",name:"MM-BrowseComp",category:"Agentic",description:"Multimodal browse + synthesize benchmark for web agents.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"hle-vl",name:"HLE-VL",category:"Agentic",description:"Vision-language variant of Humanity's Last Exam under agentic settings.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"scicode",name:"SciCode",category:"Advanced Tasks",description:"Scientific programming benchmark for code synthesis and correctness.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"frontiersci-research",name:"FrontierSci Research",category:"Advanced Tasks",description:"Open-ended scientific research benchmark with expert-level questions.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"biobench",name:"BioBench",category:"Advanced Tasks",description:"Biology and life-science benchmark requiring deep domain reasoning.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"ainstein-bench",name:"AInstein-Bench",category:"Advanced Tasks",description:"Hard scientific reasoning benchmark inspired by olympiad-level tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"vibe-coding",name:"Vibe Coding",category:"Advanced Tasks",description:"High-level coding outcome quality benchmark for agent-driven development.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"nl2repo-bench",name:"NL2Repo-Bench",category:"Advanced Tasks",description:"Natural language to repository-wide code edits benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"nl2repo-pass1",name:"NL2Repo Pass@1",category:"Advanced Tasks",description:"Pass@1 metric for repository-scale code modification tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"cl-bench",name:"CL-Bench",category:"Advanced Tasks",description:"Complex language benchmark covering difficult enterprise workflows.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-complex",name:"TOB Complex",category:"Advanced Tasks",description:"Task-oriented benchmark for complex instruction execution.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-reference",name:"TOB Reference",category:"Advanced Tasks",description:"Reference-heavy task-oriented benchmark requiring retrieval fidelity.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"healthbench-hard",name:"HealthBench Hard",category:"Advanced Tasks",description:"Hard-split medical reasoning benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"gdpval-diamond",name:"GDPVal Diamond",category:"Advanced Tasks",description:"Diamond subset for difficult planning and valuation tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"xpert-bench",name:"Xpert-Bench",category:"Advanced Tasks",description:"Expert-level evaluation benchmark across specialist domains.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-k12",name:"TOB K12",category:"Advanced Tasks",description:"Task-oriented benchmark for K12 educational tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-compositional",name:"TOB Compositional",category:"Advanced Tasks",description:"Compositional instruction-following benchmark with chained constraints.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-classification",name:"TOB Classification",category:"Advanced Tasks",description:"Classification-focused track of task-oriented benchmark suite.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tob-extraction",name:"TOB Extraction",category:"Advanced Tasks",description:"Extraction-focused benchmark for structured information tasks.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"world-travel-vlm",name:"World-Travel VLM",category:"Advanced Tasks",description:"Vision-language travel-planning and grounded reasoning benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"world-travel-text",name:"World-Travel Text",category:"Advanced Tasks",description:"Text-only travel-planning and itinerary reasoning benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"simplevqa",name:"SimpleVQA",category:"Vision",description:"Short-form visual question answering with verifiable responses.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"videommmu",name:"VideoMMMU",category:"Video",description:"Video variant of MMMU for multimodal understanding and reasoning.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"videomme",name:"VideoMME",category:"Video",description:"Video multimodal evaluation benchmark for perception and reasoning.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"tvbench",name:"TVBench",category:"Video",description:"Television/video narrative understanding benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"},{id:"ovbench",name:"OVBench",category:"Video",description:"Open-world video understanding benchmark.",maxScore:100,higherIsBetter:!0,normalization:"max",unit:"%"}].map(a=>({...a,normalization:a.normalization??("lmarena-elo"===a.id?"elo":a.higherIsBetter?"max":"inverse"),unit:a.unit??("lmarena-elo"===a.id?"ELO":"%"),link:a.link??b[a.id]??`https://artificialanalysis.ai/evaluations#${a.id}`}));a.s(["benchmarks",0,c])},9427,7921,a=>{"use strict";var b=a.i(72131);let c=function(){try{let a="nuqs-localStorage-test";if("u"<typeof localStorage)return!1;localStorage.setItem(a,a);let b=localStorage.getItem(a)===a;return localStorage.removeItem(a),b&&(localStorage.getItem("debug")||"").includes("nuqs")}catch{return!1}}();function d(a,...b){if(!c)return;let e=function(a,...b){return a.replace(/%[sfdO]/g,a=>{let c=b.shift();return"%O"===a&&c?JSON.stringify(c).replace(/"([^"]+)":/g,"$1:"):String(c)})}(a,...b);performance.mark(e);try{console.log(a,...b)}catch{console.log(e)}}function e(a,...b){c&&console.warn(a,...b)}let f={303:"Multiple adapter contexts detected. This might happen in monorepos.",404:"nuqs requires an adapter to work with your framework.",409:"Multiple versions of the library are loaded. This may lead to unexpected behavior. Currently using `%s`, but `%s` (via the %s adapter) was about to load on top.",414:"Max safe URL length exceeded. Some browsers may not be able to accept this URL. Consider limiting the amount of state stored in the URL.",422:"Invalid options combination: `limitUrlUpdates: debounce` should be used in SSR scenarios, with `shallow: false`",429:"URL update rate-limited by the browser. Consider increasing `throttleMs` for key(s) `%s`. %O",500:"Empty search params cache. Search params can't be accessed in Layouts.",501:"Search params cache already populated. Have you called `parse` twice?"};function g(a){return`[nuqs] ${f[a]}
  See https://nuqs.dev/NUQS-${a}`}function h(a){if(0===a.size)return"";let b=[];for(let[c,d]of a.entries()){let a=c.replace(/#/g,"%23").replace(/&/g,"%26").replace(/\+/g,"%2B").replace(/=/g,"%3D").replace(/\?/g,"%3F");b.push(`${a}=${d.replace(/%/g,"%25").replace(/\+/g,"%2B").replace(/ /g,"+").replace(/#/g,"%23").replace(/&/g,"%26").replace(/"/g,"%22").replace(/'/g,"%27").replace(/`/g,"%60").replace(/</g,"%3C").replace(/>/g,"%3E").replace(/[\x00-\x1F]/g,a=>encodeURIComponent(a))}`)}return"?"+b.join("&")}let i=(0,b.createContext)({useAdapter(){throw Error(g(404))}});function j(a){return({children:c,defaultOptions:d,processUrlSearchParams:e,...f})=>(0,b.createElement)(i.Provider,{...f,value:{useAdapter:a,defaultOptions:d,processUrlSearchParams:e}},c)}function k(a){let c=(0,b.useContext)(i);if(!("useAdapter"in c))throw Error(g(404));return c.useAdapter(a)}i.displayName="NuqsAdapterContext";let l=()=>(0,b.useContext)(i).defaultOptions,m=()=>(0,b.useContext)(i).processUrlSearchParams;function n(a){return{method:"throttle",timeMs:a}}function o(a){return{method:"debounce",timeMs:a}}a.s(["a",()=>m,"c",()=>d,"i",()=>l,"l",()=>e,"n",()=>j,"o",()=>h,"r",()=>k,"s",()=>g],7921);let p=n(50);function q(a){return null===a||Array.isArray(a)&&0===a.length}function r(a,b,c){if("string"==typeof a)c.set(b,a);else{for(let d of(c.delete(b),a))c.append(b,d);c.has(b)||c.set(b,"")}return c}function s(){let a=new Map;return{on(b,c){let d=a.get(b)||[];return d.push(c),a.set(b,d),()=>this.off(b,c)},off(b,c){let d=a.get(b);d&&a.set(b,d.filter(a=>a!==c))},emit(b,c){a.get(b)?.forEach(a=>a(c))}}}function t(a,b,c){let d=setTimeout(function(){a(),c.removeEventListener("abort",e)},b);function e(){clearTimeout(d),c.removeEventListener("abort",e)}c.addEventListener("abort",e)}function u(){let a=Promise;if(Promise.hasOwnProperty("withResolvers"))return Promise.withResolvers();let b=()=>{},c=()=>{};return{promise:new a((a,d)=>{b=a,c=d}),resolve:b,reject:c}}function v(){return new URLSearchParams(location.search)}var w=class{updateMap=new Map;options={history:"replace",scroll:!1,shallow:!0};timeMs=p.timeMs;transitions=new Set;resolvers=null;controller=null;lastFlushedAt=0;resetQueueOnNextPush=!1;push({key:a,query:b,options:c},e=p.timeMs){this.resetQueueOnNextPush&&(this.reset(),this.resetQueueOnNextPush=!1),d("[nuqs gtq] Enqueueing %s=%s %O",a,b,c),this.updateMap.set(a,b),"push"===c.history&&(this.options.history="push"),c.scroll&&(this.options.scroll=!0),!1===c.shallow&&(this.options.shallow=!1),c.startTransition&&this.transitions.add(c.startTransition),(!Number.isFinite(this.timeMs)||e>this.timeMs)&&(this.timeMs=e)}getQueuedQuery(a){return this.updateMap.get(a)}getPendingPromise({getSearchParamsSnapshot:a=v}){return this.resolvers?.promise??Promise.resolve(a())}flush({getSearchParamsSnapshot:a=v,rateLimitFactor:b=1,...c},e){if(this.controller??=new AbortController,!Number.isFinite(this.timeMs))return d("[nuqs gtq] Skipping flush due to throttleMs=Infinity"),Promise.resolve(a());if(this.resolvers)return this.resolvers.promise;this.resolvers=u();let f=()=>{this.lastFlushedAt=performance.now();let[b,d]=this.applyPendingUpdates({...c,autoResetQueueOnUpdate:c.autoResetQueueOnUpdate??!0,getSearchParamsSnapshot:a},e);null===d?(this.resolvers.resolve(b),this.resetQueueOnNextPush=!0):this.resolvers.reject(b),this.resolvers=null},g=()=>{let a=performance.now()-this.lastFlushedAt,c=this.timeMs,e=b*Math.max(0,c-a);d("[nuqs gtq] Scheduling flush in %f ms. Throttled at %f ms (x%f)",e,c,b),0===e?f():t(f,e,this.controller.signal)};return t(g,0,this.controller.signal),this.resolvers.promise}abort(){return this.controller?.abort(),this.controller=new AbortController,this.resolvers?.resolve(new URLSearchParams),this.resolvers=null,this.reset()}reset(){let a=Array.from(this.updateMap.keys());return d("[nuqs gtq] Resetting queue %s",JSON.stringify(Object.fromEntries(this.updateMap))),this.updateMap.clear(),this.transitions.clear(),this.options={history:"replace",scroll:!1,shallow:!0},this.timeMs=p.timeMs,a}applyPendingUpdates(a,b){let{updateUrl:c,getSearchParamsSnapshot:e}=a,f=e();if(d("[nuqs gtq] Applying %d pending update(s) on top of %s",this.updateMap.size,f.toString()),0===this.updateMap.size)return[f,null];let h=Array.from(this.updateMap.entries()),i={...this.options},j=Array.from(this.transitions);for(let[b,c]of(a.autoResetQueueOnUpdate&&this.reset(),d("[nuqs gtq] Flushing queue %O with options %O",h,i),h))null===c?f.delete(b):f=r(c,b,f);b&&(f=b(f));try{return!function(a,b){let c=b;for(let b=a.length-1;b>=0;b--){let d=a[b];if(!d)continue;let e=c;c=()=>d(e)}c()}(j,()=>{c(f,i)}),[f,null]}catch(a){return console.error(g(429),h.map(([a])=>a).join(),a),[f,a]}}};let x=new w;var y=class{callback;resolvers=u();controller=new AbortController;queuedValue=void 0;constructor(a){this.callback=a}abort(){this.controller.abort(),this.queuedValue=void 0}push(a,b){return this.queuedValue=a,this.controller.abort(),this.controller=new AbortController,t(()=>{let b=this.resolvers;try{d("[nuqs dq] Flushing debounce queue",a);let c=this.callback(a);d("[nuqs dq] Reset debounce queue %O",this.queuedValue),this.queuedValue=void 0,this.resolvers=u(),c.then(a=>b.resolve(a)).catch(a=>b.reject(a))}catch(a){this.queuedValue=void 0,b.reject(a)}},b,this.controller.signal),this.resolvers.promise}};let z=new class{throttleQueue;queues=new Map;queuedQuerySync=s();constructor(a=new w){this.throttleQueue=a}useQueuedQueries(a){var c,d;let e,f;return c=(a,b)=>this.queuedQuerySync.on(a,b),d=a=>this.getQueuedQuery(a),e=(0,b.useCallback)(()=>{let b=Object.fromEntries(a.map(a=>[a,d(a)]));return[JSON.stringify(b),b]},[a.join(","),d]),null===(f=(0,b.useRef)(null)).current&&(f.current=e()),(0,b.useSyncExternalStore)((0,b.useCallback)(b=>{let d=a.map(a=>c(a,b));return()=>d.forEach(a=>a())},[a.join(","),c]),()=>{let[a,b]=e();return f.current[0]===a?f.current[1]:(f.current=[a,b],b)},()=>f.current[1])}push(a,b,c,e){if(!Number.isFinite(b))return Promise.resolve((c.getSearchParamsSnapshot??v)());let f=a.key;if(!this.queues.has(f)){d("[nuqs dqc] Creating debounce queue for `%s`",f);let a=new y(a=>(this.throttleQueue.push(a),this.throttleQueue.flush(c,e).finally(()=>{this.queues.get(a.key)?.queuedValue===void 0&&(d("[nuqs dqc] Cleaning up empty queue for `%s`",a.key),this.queues.delete(a.key)),this.queuedQuerySync.emit(a.key)})));this.queues.set(f,a)}d("[nuqs dqc] Enqueueing debounce update %O",a);let g=this.queues.get(f).push(a,b);return this.queuedQuerySync.emit(f),g}abort(a){let b=this.queues.get(a);return b?(d("[nuqs dqc] Aborting debounce queue %s=%s",a,b.queuedValue?.query),this.queues.delete(a),b.abort(),this.queuedQuerySync.emit(a),a=>(a.then(b.resolvers.resolve,b.resolvers.reject),a)):a=>a}abortAll(){for(let[a,b]of this.queues.entries())d("[nuqs dqc] Aborting debounce queue %s=%s",a,b.queuedValue?.query),b.abort(),b.resolvers.resolve(new URLSearchParams),this.queuedQuerySync.emit(a);this.queues.clear()}getQueuedQuery(a){let b=this.queues.get(a)?.queuedValue?.query;return void 0!==b?b:this.throttleQueue.getQueuedQuery(a)}}(x);a.s(["a",()=>r,"c",()=>n,"i",()=>q,"n",()=>x,"o",()=>o,"r",()=>s,"s",()=>p,"t",()=>z],9427)}];

//# sourceMappingURL=_b63a66df._.js.map