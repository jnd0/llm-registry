1:"$Sreact.fragment"
2:I[22016,["/_next/static/chunks/50e362abf678ecbe.js","/_next/static/chunks/5a162bb283fc115e.js","/_next/static/chunks/195c216f52dae3f8.js","/_next/static/chunks/f1aa34f1a3c869ef.js","/_next/static/chunks/6b5063fbd0ddba71.js","/_next/static/chunks/ffbb017a68df0472.js","/_next/static/chunks/d2af07b8dd42c1ce.js"],""]
10:I[86831,["/_next/static/chunks/50e362abf678ecbe.js","/_next/static/chunks/5a162bb283fc115e.js","/_next/static/chunks/195c216f52dae3f8.js","/_next/static/chunks/f1aa34f1a3c869ef.js","/_next/static/chunks/6b5063fbd0ddba71.js","/_next/static/chunks/ffbb017a68df0472.js","/_next/static/chunks/d2af07b8dd42c1ce.js"],"BenchmarkExpandableList"]
11:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/a2dfb6fc5208ab9b.js"],"OutletBoundary"]
12:"$Sreact.suspense"
0:{"buildId":"ndSWmh2LFKxVirkTwO1tD","rsc":["$","$1","c",{"children":[["$","div",null,{"className":"space-y-4","children":[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Leaderboard\",\"item\":\"https://llm-registry.com/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"Domains\",\"item\":\"https://llm-registry.com/benchmarks\"},{\"@type\":\"ListItem\",\"position\":3,\"name\":\"Vision \\u0026 Video\",\"item\":\"https://llm-registry.com/domain/vision-video\"}]}"}}],["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"CollectionPage\",\"name\":\"Vision \\u0026 Video benchmarks and rankings\",\"description\":\"Evaluates visual understanding including image classification, object detection, video comprehension, and multimodal reasoning. Covers MMMU, VQA, video understanding, and cross-modal tasks.\",\"url\":\"https://llm-registry.com/domain/vision-video\",\"isPartOf\":{\"@type\":\"WebSite\",\"name\":\"LLM Registry\",\"url\":\"https://llm-registry.com\"}}"}}],["$","nav",null,{"className":"hidden sm:flex items-center gap-1 text-xs text-muted-foreground","children":[["$","$L2",null,{"href":"/","className":"hover:text-foreground transition-colors","children":"Leaderboard"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-chevron-right h-3 w-3","aria-hidden":"true","children":[["$","path","mthhwq",{"d":"m9 18 6-6-6-6"}],"$undefined"]}],["$","span",null,{"className":"text-foreground font-medium","children":"Vision & Video"}]]}],["$","div",null,{"children":[["$","h1",null,{"className":"font-display text-2xl font-bold tracking-tight text-foreground","children":"Vision & Video"}],["$","p",null,{"className":"mt-2 text-sm text-muted-foreground max-w-2xl","children":"Evaluates visual understanding including image classification, object detection, video comprehension, and multimodal reasoning. Covers MMMU, VQA, video understanding, and cross-modal tasks."}]]}],["$","div",null,{"className":"grid gap-4 lg:grid-cols-3","children":[["$","div",null,{"className":"lg:col-span-2 space-y-4","children":["$","section",null,{"className":"surface-card rounded-xl border border-border/40 p-4","children":[["$","h2",null,{"className":"font-mono text-[10px] font-bold uppercase tracking-[0.2em] text-muted-foreground mb-3","children":"Top Models"}],["$","div",null,{"className":"space-y-2","children":[["$","$L2","gpt-5-2-pro",{"href":"/model/gpt-5-2-pro","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-amber-500/20 text-amber-600 dark:text-amber-400","children":1}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"GPT-5.2 Pro"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",3]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"88.6"}]]}]]}],["$","$L2","openai-gpt-5-1",{"href":"/model/openai-gpt-5-1","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-slate-400/20 text-slate-600 dark:text-slate-300","children":2}],"$L3"]}],"$L4"]}],"$L5","$L6","$L7","$L8","$L9","$La","$Lb","$Lc"]}]]}]}],"$Ld"]}]]}],["$Le"],"$Lf"]}],"loading":null,"isPartial":false}
3:["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"GPT-5.1"}]
4:["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",1]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"84.5"}]]}]
5:["$","$L2","meta-llama-4-maverick",{"href":"/model/meta-llama-4-maverick","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-orange-700/20 text-orange-700 dark:text-orange-400","children":3}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Llama 4 Maverick"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",2]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"84.1"}]]}]]}]
6:["$","$L2","gpt-5-2-xhigh",{"href":"/model/gpt-5-2-xhigh","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":4}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"GPT-5.2 Extra High"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",5]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"83.9"}]]}]]}]
7:["$","$L2","qwen-3-5-397b-a17b",{"href":"/model/qwen-3-5-397b-a17b","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":5}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Qwen 3.5 397B-A17B"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",16]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"83.4"}]]}]]}]
8:["$","$L2","gemini-3-pro",{"href":"/model/gemini-3-pro","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":6}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Gemini 3 Pro"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",5]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"82.2"}]]}]]}]
9:["$","$L2","gemini-3-flash",{"href":"/model/gemini-3-flash","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":7}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Gemini 3 Flash"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",5]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"81.1"}]]}]]}]
a:["$","$L2","google-gemini-3-deep-think",{"href":"/model/google-gemini-3-deep-think","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":8}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Gemini 3 Deep Think"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",1]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"81.0"}]]}]]}]
b:["$","$L2","gemini-3-pro-high",{"href":"/model/gemini-3-pro-high","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":9}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Gemini 3 Pro High"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",10]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"80.5"}]]}]]}]
c:["$","$L2","gemini-3.1-pro",{"href":"/model/gemini-3.1-pro","className":"flex items-center justify-between gap-4 p-2 rounded-lg hover:bg-muted/30 transition-colors group","children":[["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"flex h-5 w-5 items-center justify-center rounded text-[10px] font-bold bg-muted text-muted-foreground","children":10}],["$","span",null,{"className":"font-medium text-foreground group-hover:text-primary transition-colors","children":"Gemini 3.1 Pro"}]]}],["$","div",null,{"className":"flex items-center gap-3","children":[["$","span",null,{"className":"text-xs text-muted-foreground font-mono","children":["n=",1]}],["$","span",null,{"className":"font-mono text-sm font-bold text-foreground","children":"80.5"}]]}]]}]
d:["$","div",null,{"className":"space-y-4","children":[["$","section",null,{"className":"surface-card rounded-xl border border-border/40 p-4","children":[["$","h2",null,{"className":"font-mono text-[10px] font-bold uppercase tracking-[0.2em] text-muted-foreground mb-3","children":"Domain Info"}],["$","dl",null,{"className":"space-y-2.5 text-sm","children":[["$","div",null,{"className":"flex justify-between","children":[["$","dt",null,{"className":"text-muted-foreground","children":"Benchmarks"}],["$","dd",null,{"className":"font-mono text-foreground","children":88}]]}],["$","div",null,{"className":"flex justify-between","children":[["$","dt",null,{"className":"text-muted-foreground","children":"Models Evaluated"}],["$","dd",null,{"className":"font-mono text-foreground","children":34}]]}],["$","div",null,{"className":"flex justify-between","children":[["$","dt",null,{"className":"text-muted-foreground","children":"Categories"}],["$","dd",null,{"className":"text-foreground","children":"Vision, Video, Multimodal"}]]}]]}]]}],["$","section",null,{"className":"surface-card rounded-xl border border-border/40 p-4","children":[["$","h2",null,{"className":"font-mono text-[10px] font-bold uppercase tracking-[0.2em] text-muted-foreground mb-3","children":"Benchmarks"}],["$","$L10",null,{"benchmarks":[{"id":"mmmu","name":"MMMU (Multimodal)","category":"Multimodal","description":"Multi-discipline Multimodal Understanding and Reasoning.","maxScore":100,"higherIsBetter":true,"link":"https://mmmu-benchmark.github.io/","paperUrl":"https://arxiv.org/abs/2311.16502","normalization":"max","unit":"%"},{"id":"mathvista","name":"MathVista","category":"Vision","description":"Mathematical reasoning in visual contexts.","maxScore":100,"higherIsBetter":true,"link":"https://mathvista.github.io/","paperUrl":"https://arxiv.org/abs/2310.02255","normalization":"max","unit":"%"},{"id":"mathvista-mini","name":"MathVista (mini)","category":"Vision","description":"Compact MathVista split for faster multimodal reasoning checks.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://mathvista.github.io/"},{"id":"mathvision","name":"MathVision","category":"Vision","description":"Comprehensive mathematical vision benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mathvision"},{"id":"mmmu-vision","name":"MMMU","category":"Vision","description":"Massive Multi-discipline Multimodal Understanding and Reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://mmmu-benchmark.github.io/"},{"id":"logicvista","name":"LogicVista","category":"Vision","description":"Logical reasoning in visual puzzles and diagrams.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#logicvista"},{"id":"blink","name":"BLINK","category":"Vision","description":"Spatial and perception benchmark for multimodal models.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#blink"},{"id":"mmvp","name":"MMVP","category":"Vision","description":"Multimodal visual perception benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmvp"},{"id":"chartqapro","name":"ChartQA Pro","category":"Vision","description":"Expert-level chart understanding and question answering.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#chartqapro"},{"id":"docvqa","name":"DocVQA","category":"Vision","description":"Document visual question answering on scanned and digital documents.","maxScore":100,"higherIsBetter":true,"link":"https://rrc.cvc.uab.es/?ch=17","paperUrl":"https://arxiv.org/abs/2007.00398","normalization":"max","unit":"%"},{"id":"ocrbench-v2","name":"OCRBench v2","category":"Vision","description":"Next-gen optical character recognition and document understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://ocrbench.github.io/"},{"id":"ocrbench","name":"OCRBench","category":"Vision","description":"Optical character recognition and document understanding benchmark.","maxScore":100,"higherIsBetter":true,"link":"https://ocrbench.github.io/","paperUrl":"https://arxiv.org/abs/2312.16151","normalization":"max","unit":"%"},{"id":"dynamath","name":"DynaMath","category":"Vision","description":"Dynamic mathematical reasoning in visual contexts.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#dynamath"},{"id":"mathkangaroo","name":"MathKangaroo","category":"Vision","description":"Mathematical competition problems with visual elements.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mathkangaroo"},{"id":"mathcanvas","name":"MathCanvas","category":"Vision","description":"Multi-step mathematical reasoning on a canvas.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mathcanvas"},{"id":"mmmu-pro","name":"MMMU-Pro","category":"Vision","description":"Professional level MMMU expansion.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://mmmu-benchmark.github.io/"},{"id":"mmmu-val","name":"MMMU (val)","category":"Vision","description":"Validation split of MMMU for multimodal understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://mmmu-benchmark.github.io/"},{"id":"emma","name":"EMMA","category":"Vision","description":"Expert-level Multimodal Mathematics Analysis.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#emma"},{"id":"sfe","name":"SFE","category":"Vision","description":"Scientific Figure Evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#sfe"},{"id":"hipho","name":"HiPhO","category":"Vision","description":"High-level Physics Olympiad (Vision).","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#hipho"},{"id":"xlrs-bench","name":"XLRS-Bench","category":"Vision","description":"Cross-domain Logical Reasoning and Spatial benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#xlrs-bench"},{"id":"phyx","name":"PhyX","category":"Vision","description":"Physics reasoning with open-ended visual questions.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#phyx"},{"id":"vpct","name":"VPCT","category":"Vision","description":"Visual Perception and Coding Tasks.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#vpct"},{"id":"zerobench-main","name":"ZeroBench (main)","category":"Vision","description":"Zero-shot visual reasoning benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#zerobench-main"},{"id":"zerobench-sub","name":"ZeroBench (sub)","category":"Vision","description":"Zero-shot visual reasoning sub-tasks.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#zerobench-sub"},{"id":"zerobench","name":"ZeroBench","category":"Vision","description":"Aggregate ZeroBench score across the full task set.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#zerobench"},{"id":"zerobench-tools","name":"ZeroBench (w/ tools)","category":"Vision","description":"ZeroBench score when tool use is allowed.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#zerobench-tools"},{"id":"arc-agi-1-image","name":"ArcAGI1-Image","category":"Vision","description":"ARC-AGI Level 1 tasks in image format.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://arcprize.org/"},{"id":"arc-agi-2-image","name":"ArcAGI2-Image","category":"Vision","description":"ARC-AGI Level 2 tasks in image format.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://arcprize.org/"},{"id":"visulogic","name":"VisuLogic","category":"Vision","description":"Visual logic and sequence reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#visulogic"},{"id":"vlms-are-biased","name":"VLMsAreBiased","category":"Vision","description":"Evaluating bias in Vision-Language Models.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#vlms-are-biased"},{"id":"vlms-are-blind","name":"VLMsAreBlind","category":"Vision","description":"Evaluating perception failures in VLMs.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#vlms-are-blind"},{"id":"visfactor","name":"VisFactor","category":"Vision","description":"Visual factor identification and reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#visfactor"},{"id":"realworldqa","name":"RealWorldQA","category":"Vision","description":"Real-world visual question answering.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#realworldqa"},{"id":"babyvision","name":"BabyVision","category":"Vision","description":"Early-stage visual development benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#babyvision"},{"id":"hallusionbench","name":"HallusionBench","category":"Vision","description":"Visual hallucination and factuality benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#hallusionbench"},{"id":"mme-cc","name":"MME-CC","category":"Vision","description":"Multimodal Evaluation (Cognitive Capacity).","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mme-cc"},{"id":"mmstar","name":"MMStar","category":"Vision","description":"Elite multimodal model evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmstar"},{"id":"muirbench","name":"MUIRBench","category":"Vision","description":"Multimodal Understanding and Interaction Benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#muirbench"},{"id":"mtvqa","name":"MTVQA","category":"Vision","description":"Multilingual Text-centric Visual QA.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mtvqa"},{"id":"worldvqa","name":"WorldVQA","category":"Vision","description":"Global visual knowledge and reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#worldvqa"},{"id":"vibeeval","name":"VibeEval","category":"Vision","description":"Subjective and intuitive visual quality evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#vibeeval"},{"id":"viverbench","name":"ViVerBench","category":"Vision","description":"Visual Verification and reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#viverbench"},{"id":"countbench","name":"CountBench","category":"Vision","description":"Visual object counting and identification.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#countbench"},{"id":"fsc-147","name":"FSC-147","category":"Vision","description":"Few-shot counting benchmark (Lower is better handled in normalization).","maxScore":100,"higherIsBetter":false,"normalization":"inverse","unit":"error","link":"https://artificialanalysis.ai/evaluations#fsc-147"},{"id":"point-bench","name":"Point-Bench","category":"Vision","description":"Visual pointing and spatial grounding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#point-bench"},{"id":"mmsibench","name":"MMSIBench","category":"Vision","description":"Multimodal Spatial Interaction Benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmsibench"},{"id":"treebench","name":"TreeBench","category":"Vision","description":"Hierarchical visual reasoning tasks.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#treebench"},{"id":"refspatialbench","name":"RefSpatialBench","category":"Vision","description":"Referential spatial reasoning evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#refspatialbench"},{"id":"da-2k","name":"DA-2K","category":"Vision","description":"Document Analysis and reasoning (2k).","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#da-2k"},{"id":"all-angles","name":"All-Angles","category":"Vision","description":"Multi-perspective visual understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#all-angles"},{"id":"erqa","name":"ERQA","category":"Vision","description":"Environment Reasoning and Question Answering.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#erqa"},{"id":"omnidocbench","name":"OmniDocBench","category":"Vision","description":"Universal document understanding benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#omnidocbench"},{"id":"omnidocbench-15","name":"OmniDocBench 1.5","category":"Vision","description":"OCR benchmark measuring edit distance (lower is better).","maxScore":1,"minScore":0,"higherIsBetter":false,"unit":"edit distance","normalization":"inverse","link":"https://artificialanalysis.ai/evaluations#omnidocbench-15"},{"id":"screenspot-pro","name":"ScreenSpot-Pro","category":"Vision","description":"Screen understanding benchmark for GUI interaction.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://github.com/octopus-tools/screenspot-pro"},{"id":"infovqa-test","name":"InfoVQA (test)","category":"Vision","description":"Information-seeking visual question answering on the test split.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#infovqa-test"},{"id":"charxiv-dq","name":"CharXiv-DQ","category":"Vision","description":"Chart-based reasoning from arXiv papers (Data QA).","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#charxiv-dq"},{"id":"charxiv-rq","name":"CharXiv-RQ","category":"Vision","description":"Chart-based reasoning from arXiv papers (Reasoning QA).","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#charxiv-rq"},{"id":"charxiv-reasoning","name":"CharXiv Reasoning","category":"Vision","description":"Information synthesis from complex charts.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://arxiv.org/abs/2406.18521"},{"id":"dude","name":"DUDE","category":"Vision","description":"Document Understanding and Dialogue Evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#dude"},{"id":"mmlongbench","name":"MMLongBench","category":"Vision","description":"Multimodal Long context benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmlongbench"},{"id":"longdocurl","name":"LongDocURL","category":"Vision","description":"Long document understanding with URLs.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#longdocurl"},{"id":"mmlongbench-doc","name":"MMLongBench-Doc","category":"Vision","description":"Multimodal Long context document evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmlongbench-doc"},{"id":"mmvu","name":"MMVU","category":"Video","description":"Multimodal Video Understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#mmvu"},{"id":"videosimpleqa","name":"VideoSimpleQA","category":"Video","description":"Verifiable question answering for short video clips.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#videosimpleqa"},{"id":"videoreasonbench","name":"VideoReasonBench","category":"Video","description":"Complex reasoning tasks in video content.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#videoreasonbench"},{"id":"morse-500","name":"Morse-500","category":"Video","description":"Sequence reasoning and motion understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#morse-500"},{"id":"videoholmes","name":"VideoHolmes","category":"Video","description":"Deep diagnostic video understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#videoholmes"},{"id":"minerva","name":"Minerva","category":"Video","description":"Long-form video reasoning and knowledge retrieval.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#minerva"},{"id":"contphy","name":"ContPhy","category":"Video","description":"Continuous Physics reasoning in video.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#contphy"},{"id":"tempcompass","name":"TempCompass","category":"Video","description":"Temporal orientation and perception in video.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#tempcompass"},{"id":"egotempo","name":"EgoTempo","category":"Video","description":"First-person perspective temporal reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#egotempo"},{"id":"motionbench","name":"MotionBench","category":"Video","description":"Comprehensive motion perception evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#motionbench"},{"id":"tomato","name":"TOMATO","category":"Video","description":"Temporal Object-centric Multimodal Analysis.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#tomato"},{"id":"cgbench","name":"CGBench","category":"Video","description":"Contextual Grounding in long videos.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#cgbench"},{"id":"longvideobench","name":"LongVideoBench","category":"Video","description":"Understanding extremely long-form video content.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#longvideobench"},{"id":"videoeval-pro","name":"VideoEval-Pro","category":"Video","description":"Professional level video quality and content evaluation.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#videoeval-pro"},{"id":"lvbench","name":"LVBench","category":"Video","description":"Large-scale Video Benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#lvbench"},{"id":"crossvid","name":"CrossVid","category":"Video","description":"Cross-video temporal and relational reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#crossvid"},{"id":"livesports-3k","name":"LiveSports-3K","category":"Video","description":"Live sports broadcast understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#livesports-3k"},{"id":"ovobench","name":"OVOBench","category":"Video","description":"Object-Video-Object relational reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#ovobench"},{"id":"odvbench","name":"ODVBench","category":"Video","description":"Open-Domain Video understanding.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#odvbench"},{"id":"vispeak","name":"ViSpeak","category":"Video","description":"Video-to-speech and dialogue reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#vispeak"},{"id":"simplevqa","name":"SimpleVQA","category":"Vision","description":"Short-form visual question answering with verifiable responses.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#simplevqa"},{"id":"videommmu","name":"VideoMMMU","category":"Video","description":"Video variant of MMMU for multimodal understanding and reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#videommmu"},{"id":"videomme","name":"VideoMME","category":"Video","description":"Video multimodal evaluation benchmark for perception and reasoning.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://video-mme.github.io/"},{"id":"tvbench","name":"TVBench","category":"Video","description":"Television/video narrative understanding benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#tvbench"},{"id":"ovbench","name":"OVBench","category":"Video","description":"Open-world video understanding benchmark.","maxScore":100,"higherIsBetter":true,"normalization":"max","unit":"%","link":"https://artificialanalysis.ai/evaluations#ovbench"}]}]]}]]}]
e:["$","script","script-0",{"src":"/_next/static/chunks/d2af07b8dd42c1ce.js","async":true}]
f:["$","$L11",null,{"children":["$","$12",null,{"name":"Next.MetadataOutlet","children":"$@13"}]}]
13:null
